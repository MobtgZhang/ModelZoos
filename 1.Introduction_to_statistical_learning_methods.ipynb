{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83250ab9",
   "metadata": {},
   "source": [
    "# 第1章:统计学习及监督学习概论\n",
    "## 1. 统计学习当中一些重要的概念或者定义\n",
    "+ 统计学习是关于计算机基于数据结构概率统计模型并且运用模型对数据进行预测与分析的一门学科,统计学习也称为统计机器学习.\n",
    "+ 统计学习包括有监督学习、非监督学习、半监督学习和强化学习.\n",
    "+ 统计学习的三要素:模型、策略和算法\n",
    "+ 监督学习:指的是从标注数据中学习预测模型的机器学习问题.标注的数据表示输入输出的对应关系,预测模型对给定的输入产生相应的输出.监督学习的本质是学习输入到输出映射的统计规律.\n",
    "## 2. 监督学习\n",
    "+ **输入空间、输出空间和特征空间**：在监督学习当中,将输入和输出所有可能取值的集合分别称为输入空间和输出空间.输入空间空间与输出空间可以是有限或者无线的集合.输出变量$X$,输出变量为$Y$,输出实例$x$的特征向量记作以下的形式:$$x=\\left(x^{(1)},x^{(2)},\\cdots,x^{(i)},\\cdots,x^{(n)}\\right)$$\n",
    "  训练集一般表示为以下的形式$$T=\\left\\{(x_{1},y_{1}),(x_{2},y_{2}),\\cdots,(x_{i},y_{i}),\\cdots,(x_{N},y_{N})\\right\\}$$\n",
    "+ **联合概率分布**：监督学习假设输入与输出的随机变量$X,Y$遵循联合概率分布$P(X,Y)$,它表示分布函数或者说是分布密度函数,注意在学习过程当中,假定这一联合概率分布存在.一般情况下,训练数据与测试数据被看做是依照联合概率分布$P(X,Y)$独立同分布来产生的.统计学习假设数据存在与一定的统计规律,$X,Y$具有联合概率分布就是监督学习关于数据的基本假设.\n",
    "+ **假设空间**：监督学习的目的在于学习一个由输入到输出的映射，这一映射由模型来表示。模型属于由输入空间到输出空间的映射集合，这个集合就是假设空间。监督学习模型可以概率模型或者是非概率模型通常由条件概率分布$P\\left(Y\\left|\\right.X\\right)$或者是决策函数$Y=f(X)$表示.\n",
    "+ **问题的形式化**：学习系统、预测系统和模型三个部分组成.\n",
    "## 3. 非监督学习\n",
    "&emsp;&emsp;指的是从无标注数据中心学习预测模型的机器学习问题.无标注数据是自然得到的数据,预测模型表示数据的类别、转换或者概率.无监督学习的本质是学习数据中的统计规律或者潜在的结构.若$\\mathcal{X}$是输入空间,$\\mathcal{Z}$表示的是隐式结构空间.那么要学习的模型就可以表示为以下的函数形式$z=g(x)$,条件概率分布$P\\left(z\\left|\\right.x\\right)$或者是条件概率分布$P\\left(x\\left|\\right.z\\right)$ 的形式,$x\\in{\\mathcal{X}}$是输入,$z\\in{\\mathcal{Z}}$表示的是输出.训练数据集一般表示为$U=\\left\\{x_{1},x_{2},\\cdots,x_{N}\\right\\},x_{i},i=1,2,\\cdots,N$.无监督学习通常使用的是大量未经过标注的数据进行学习和训练,分析时候使用到已经学习好的模型即函数$z=\\hat{g}(x)$,条件概率分布$\\hat{P}\\left(z\\left|\\right.x\\right)$或者$\\hat{P}\\left(x\\left|\\right.z\\right)$.这样给定对应的输入$x_{N+1}$,那么就可以得到模型给出的$z_{N+1}=\\hat{g}(x_{N+1})$或者$z_{N+1}=\\arg\\limits\\max_{z}\\hat{P}\\left(z\\left|\\right.x_{N+1}\\right)$\n",
    "## 4. 强化学习\n",
    "&emsp;&emsp;指智能系统在与环境的连续互动中学习最优行为策略的机器学习为。强化学习的本质是学习最优的序贯策略.强化学习的马尔科夫决策过程是状态、奖励、动作序列上的随机过程，由五元组$(S,A,P,r,\\gamma)$组成\n",
    "+ S是有限状态的集合\n",
    "+ A是有限动作的集合\n",
    "+ P是状态转移概率函数:$$P\\left(s^{\\prime}\\left|s,a\\right.\\right)=P\\left(s_{t+1}\\left|s_{t}=s,a_{t}=a\\right.\\right)$$\n",
    "+ r是奖励函数:$$r(r,a)=E\\left(r_{t+1}\\left|s_{t}=s,a_{t}=a\\right.\\right)$$\n",
    "+ $\\gamma$是衰减系数:$\\gamma\\in[0,1]$\n",
    "\n",
    "马尔科夫决策过程具有马尔科夫性,下一个状态只依赖于前一个状态与动作,由状态转移概率函数$P\\left(s^{\\prime}\\left|s,a\\right.\\right)$表示.下一个奖励依赖于前一个状态与动作,由奖励函数$r(s,a)$表示.\n",
    "\n",
    "策略$\\pi$定义为给定状态动作的函数$a=f(s)$或者条件概率分布$P\\left(a\\left|s\\right.\\right)$.给定一个策略$\\pi$,智能系统与环境之间的互动行为就已经确定了起来(随机性或者是确定性的).\n",
    "\n",
    "价值函数或者状态价值函数定义为策略$\\pi$从某一个状态$s$开始的长期积累奖励的数学期望$$v_{\\pi}(s)=\\mathbb{E}_{\\pi}\\left[r_{t+1}+\\gamma{r_{t+2}}+\\gamma^{2}{r_{t+2}}+\\cdots\\left|s_{t}=s\\right.\\right]$$\n",
    "\n",
    "动作价值函数定义为策略$\\pi$的某一个状态$s$和动作$a$开始的长期积累奖励的数学期望:$$q_{\\pi}(s,a)=\\mathbb{E}_{\\pi}\\left[r_{t+1}+\\gamma{r_{t+2}}+\\gamma^{2}{r_{t+2}}+\\cdots\\left|s_{t}=s,a_{t}=a\\right.\\right]$$\n",
    "\n",
    "强化学习目标就是在所有可能的策略当中选出价值函数的最大策略$\\pi^{*}$,而在实际学习中往往从具体的策略出发,不断优化已有策略,这里$\\gamma$表示未来的奖励有衰减.\n",
    "\n",
    "强化学习放放中有基于策略的、基于价值的(无模型方法),还有有模型方法.无模型的、基于策略的方法不直接学习模型,而是试图求解最优策略$\\pi^{*}$,表示为函数$a=f^{*}(s)$或者是条件概率分布$P^{*}(a\\left|s\\right.)$,这样也能达到在环境中做出最优策略决策的目的.学习通常从一个具体策略开始,通过搜索更优的策略进行.\n",
    "## 5. 半监督学习\n",
    "半监督学习是利用标注数据和未标注数据学习预测模型的机器学习问题,通常是有少量的数据、大量的未标注数据,标注数据构建往往需要人工构建,成本比较高,未标注的数据收集通常来说不需要太高的成本,这样通过半监督学习的方法可以进行以较低的成本达到较好学习的效果.\n",
    "## 6. 主动学习\n",
    "主动学习指的是机器不断主动给出实例让教师进行标注,然后利用标注数据学习预测模型机器学习的问题."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56434d6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.1",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
