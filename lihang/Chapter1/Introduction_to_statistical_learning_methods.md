# 第1章:统计学习及监督学习概论
## 1. 统计学习当中一些重要的概念或者定义
+ 统计学习是关于计算机基于数据结构概率统计模型并且运用模型对数据进行预测与分析的一门学科,统计学习也称为统计机器学习.
+ 统计学习包括有监督学习、非监督学习、半监督学习和强化学习.
+ 统计学习的三要素:模型、策略和算法
+ 监督学习:指的是从标注数据中学习预测模型的机器学习问题.标注的数据表示输入输出的对应关系,预测模型对给定的输入产生相应的输出.监督学习的本质是学习输入到输出映射的统计规律.
## 2. 一般统计学习模型分类
### 2.1 监督学习
+ **输入空间、输出空间和特征空间**：在监督学习当中,将输入和输出所有可能取值的集合分别称为输入空间和输出空间.输入空间空间与输出空间可以是有限或者无线的集合.输出变量$X$,输出变量为$Y$,输出实例$x$的特征向量记作以下的形式:$$x=\left(x^{(1)},x^{(2)},\cdots,x^{(i)},\cdots,x^{(n)}\right)$$
  训练集一般表示为以下的形式$$T=\left\{(x_{1},y_{1}),(x_{2},y_{2}),\cdots,(x_{i},y_{i}),\cdots,(x_{N},y_{N})\right\}$$
+ **联合概率分布**：监督学习假设输入与输出的随机变量$X,Y$遵循联合概率分布$P(X,Y)$,它表示分布函数或者说是分布密度函数,注意在学习过程当中,假定这一联合概率分布存在.一般情况下,训练数据与测试数据被看做是依照联合概率分布$P(X,Y)$独立同分布来产生的.统计学习假设数据存在与一定的统计规律,$X,Y$具有联合概率分布就是监督学习关于数据的基本假设.
+ **假设空间**：监督学习的目的在于学习一个由输入到输出的映射，这一映射由模型来表示。模型属于由输入空间到输出空间的映射集合，这个集合就是假设空间。监督学习模型可以概率模型或者是非概率模型通常由条件概率分布$P\left(Y\left|\right.X\right)$或者是决策函数$Y=f(X)$表示.
+ **问题的形式化**：学习系统、预测系统和模型三个部分组成.
### 2.2 非监督学习
&emsp;&emsp;指的是从无标注数据中心学习预测模型的机器学习问题.无标注数据是自然得到的数据,预测模型表示数据的类别、转换或者概率.无监督学习的本质是学习数据中的统计规律或者潜在的结构.若$\mathcal{X}$是输入空间,$\mathcal{Z}$表示的是隐式结构空间.那么要学习的模型就可以表示为以下的函数形式$z=g(x)$,条件概率分布$P\left(z\left|\right.x\right)$或者是条件概率分布$P\left(x\left|\right.z\right)$ 的形式,$x\in{\mathcal{X}}$是输入,$z\in{\mathcal{Z}}$表示的是输出.训练数据集一般表示为$U=\left\{x_{1},x_{2},\cdots,x_{N}\right\},x_{i},i=1,2,\cdots,N$.无监督学习通常使用的是大量未经过标注的数据进行学习和训练,分析时候使用到已经学习好的模型即函数$z=\hat{g}(x)$,条件概率分布$\hat{P}\left(z\left|\right.x\right)$或者$\hat{P}\left(x\left|\right.z\right)$.这样给定对应的输入$x_{N+1}$,那么就可以得到模型给出的$z_{N+1}=\hat{g}(x_{N+1})$或者$z_{N+1}=\arg\limits\max_{z}\hat{P}\left(z\left|\right.x_{N+1}\right)$
### 2.3 强化学习
&emsp;&emsp;指智能系统在与环境的连续互动中学习最优行为策略的机器学习为。强化学习的本质是学习最优的序贯策略.强化学习的马尔科夫决策过程是状态、奖励、动作序列上的随机过程，由五元组$(S,A,P,r,\gamma)$组成
+ S是有限状态的集合
+ A是有限动作的集合
+ P是状态转移概率函数:$$P\left(s^{\prime}\left|s,a\right.\right)=P\left(s_{t+1}\left|s_{t}=s,a_{t}=a\right.\right)$$
+ r是奖励函数:$$r(r,a)=E\left(r_{t+1}\left|s_{t}=s,a_{t}=a\right.\right)$$
+ $\gamma$是衰减系数:$\gamma\in[0,1]$

&emsp;&emsp;马尔科夫决策过程具有马尔科夫性,下一个状态只依赖于前一个状态与动作,由状态转移概率函数$P\left(s^{\prime}\left|s,a\right.\right)$表示.下一个奖励依赖于前一个状态与动作,由奖励函数$r(s,a)$表示.

&emsp;&emsp;策略$\pi$定义为给定状态动作的函数$a=f(s)$或者条件概率分布$P\left(a\left|s\right.\right)$.给定一个策略$\pi$,智能系统与环境之间的互动行为就已经确定了起来(随机性或者是确定性的).

&emsp;&emsp;价值函数或者状态价值函数定义为策略$\pi$从某一个状态$s$开始的长期积累奖励的数学期望$$v_{\pi}(s)=\mathbb{E}_{\pi}\left[r_{t+1}+\gamma{r_{t+2}}+\gamma^{2}{r_{t+2}}+\cdots\left|s_{t}=s\right.\right]$$

&emsp;&emsp;动作价值函数定义为策略$\pi$的某一个状态$s$和动作$a$开始的长期积累奖励的数学期望:$$q_{\pi}(s,a)=\mathbb{E}_{\pi}\left[r_{t+1}+\gamma{r_{t+2}}+\gamma^{2}{r_{t+2}}+\cdots\left|s_{t}=s,a_{t}=a\right.\right]$$

&emsp;&emsp;强化学习目标就是在所有可能的策略当中选出价值函数的最大策略$\pi^{*}$,而在实际学习中往往从具体的策略出发,不断优化已有策略,这里$\gamma$表示未来的奖励有衰减.

&emsp;&emsp;强化学习放放中有基于策略的、基于价值的(无模型方法),还有有模型方法.无模型的、基于策略的方法不直接学习模型,而是试图求解最优策略$\pi^{*}$,表示为函数$a=f^{*}(s)$或者是条件概率分布$P^{*}(a\left|s\right.)$,这样也能达到在环境中做出最优策略决策的目的.学习通常从一个具体策略开始,通过搜索更优的策略进行.
### 2.4 半监督学习
&emsp;&emsp;半监督学习是利用标注数据和未标注数据学习预测模型的机器学习问题,通常是有少量的数据、大量的未标注数据,标注数据构建往往需要人工构建,成本比较高,未标注的数据收集通常来说不需要太高的成本,这样通过半监督学习的方法可以进行以较低的成本达到较好学习的效果.
### 2.5 主动学习
&emsp;&emsp;主动学习指的是机器不断主动给出实例让教师进行标注,然后利用标注数据学习预测模型机器学习的问题.
### 2.6 按照模型分类
1. 概率模型与非概率模型:统计学习模型可以分为概率模型和非概率模型,或者是确定性模型.在监督学习当中,概率模型取条件概率分布形式$P\left(y\left|x\right.\right)$,非概率模型取函数形式$y=f(x)$,其中$x$表示的是输入,$y$表示的是输出.在无监督模型当中概率模型取条件概率分布的形式$P\left(z\left|x\right.\right)$或者说$P\left(x\left|z\right.\right)$,非概率模型取
函数形式$z=g(x)$,概率模型是生成模型,非概率模型是判别模型.在以下的几个章节当中决策树、朴素贝叶斯、隐马尔科夫模型、条件随机场、概率潜在语义分析、潜在狄利克雷分配、高斯混合模型是概率模型.感知机、支持向量机、k近邻、AdaBoost、k均值、潜在语义分析,以及神经网络都是非概率模型. 
2. 线性模型与非线性模型
3. 参数化模型与非参数化模型:参数化模型假设模型参数的维度固定,模型可以由有限维度参数完全刻画;非参数化模型假设模型参数的维度不是固定的或者是无穷大,随着训练数据量的增加而不断增大.
### 2.7 按照算法分类
&emsp;&emsp;统计学习根据算法,可以分为在线学习与批量学习.在线学习是指每次接受一个样本,进行预测,之后学习模型,并不断重复该操作的机器学习.
+ **批量学习**：一次接受所有数据,学习模型,之后进行预测.
+ **在线学习**：数据量并不是一次性接受所有的数据,而是一个个数据进行训练.
### 2.8 按技巧分类
+ **贝叶斯学习**：贝叶斯学习又称为贝叶斯推理,是统计学、机器学习中重要的方法.其主要想法是,在概率模型的学习和推理当中,利用贝叶斯定理,计算在给定数据条件下的条件概率(后验概率),并应用这个原理进行模型的估计,以及对数据的预测.朴素贝叶斯、潜在狄利克雷分配的学习属于贝叶斯学习.设随机变量$D$表示数据,随机变量$\theta$表示的是模型参数.依据贝叶斯定理,可以使用以下的公式计算后验概率$P\left(\theta\left|D\right.\right)$:$$P\left(\theta\left|D\right.\right)=\dfrac{P(\theta)P\left(D\left|\theta\right.\right)}{P(D)}$$其中$P(\theta)$表示的是先验概率,$P\left(D\left|\theta\right.\right)$是似然函数.在预测的时候计算数据对后验概率分布的期望值:$$P\left(x\left|D\right.\right)=\int{P\left(x\left|\theta,D\right.\right)P\left(\theta\left|D\right.\right)}d\theta$$
这里的$x$是新样本.极大似然估计和贝叶斯估计有以下的区别$$\hat\theta=\arg\limits\max_{\theta}P\left(D\left|\theta\right.\right)$$
$$\hat{P}\left(D\left|\theta\right.\right)=\dfrac{P(\theta)P\left(D\left|\theta\right.\right)}{P(D)}$$
+ **核方法**：核方法是使用核函数表示和学习非线性模型的一种机器学习方法，可以用于监督学习和无监督学习.将线性模型扩展到非线性模型,直接的做法是显式地定义从输入空间(低维空间)到特征空间的映射,在特征空间中进行内积计算.
## 3.统计学习方法三要素
### 3.1 模型
假设空间用$\mathcal{F}$,假设空间可以定义为决策函数的集合$$\mathcal{F}=\left\{f\left|Y=f(X)\right.\right\}$$其中,$X,Y$是定义在输入空间$\mathcal{X}$和输出空间$\mathcal{Y}$上的变量,这时$\mathcal{F}$通常是有一个参数向量决定的函数族$$\mathcal{F}=\left\{f\left|Y=f_{\theta}\left(X\right),\theta\in\mathbb{R}^{n}\right.\right\}$$
参数向量$\theta$取值于$n$维欧式空间$\mathbb{R}^{n}$,也称为参数空间.
### 3.2 策略
通过按照什么样的准则学习或者选择最优的模型,统计学习的目标在于从假设空间中选取最优模型.
1. 损失函数和风险函数

&emsp;&emsp;监督学习问题是在假设空间$\mathcal{F}$中选取模型$f$作为决策函数,对于给定的输入$X$,由$f(X)$给出相应的输出$Y$,这个输出预测值$f(X)$与真实值之间错误程度一般使用损失函数来表示,记作$L\left(Y,f(X)\right)$.

&emsp;&emsp;当然损失函数的值越小,那么模型的效果也就越好,由于输入输出的$(X,Y)$是随机变量,遵循联合分布$P(X,Y)$,所以损失函数的期望也就是以下的形式$$R_{\exp}(f)=E_{P}\left[L(Y,f(X))\right]=\int_{\mathcal{X}\times\mathcal{Y}}L\left(y,f(x)\right)P(x,y)dxdy$$
上述函数一般称为风险函数或者是期望损失.统计学习的目标就是让期望损失值变得最小. 

&emsp;&emsp;下面的损失函数一般表示为经验损失,记作以下的形式$$R_{emp}=\dfrac{1}{N}\sum\limits_{i=1}^{N}L\left(y_{i},f(x_{i})\right)$$
期望风险$R_{\exp}(f)$是模型关于联合分布的期望损失,经验风险$R_{emp}(f)$是模型关于训练样本集的平均损失.

&emsp;&emsp;结构风险最小化是为了放置过拟合而提出来的一种策略,结构风险最小化等价于正则化,结构风险在经验风险上加上表示模型复杂度的正则化项或者是惩罚项.在假设空间、损失函数以及训练数据集确定的情况下,结构风险一般定义为以下的形式$$R_{srm}(f)\dfrac{1}{N}\sum\limits_{i=1}^{N}L\left(y_{i},f(x_{i})\right)+\lambda{J(f)}$$
### 3.3 算法
&emsp;&emsp;算法指的就是学习模型具体计算方法.统计学习基于训练数据集,根据学习策略,从假设空间当中选择最优模型,最后需要考虑用什么样的计算方法求解最优模型.
## 4.模型选择与模型估计



